# -*- coding: utf-8 -*-
"""Copy of week-4_project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ladc0CFEHZY-syShEKVbrZ0JX2VHWYAc
"""

!pip install -q datasets==2.14.6 transformers
!pip install -q fsspec==2023.10.0 gcsfs==2023.10.0

from datasets import load_dataset

dataset = load_dataset("wikitext", name="wikitext-2-raw-v1")
train_data = dataset["train"]
val_data = dataset["validation"]

print(f"âœ… Loaded: {len(train_data)} training samples")
print(f"ðŸ§¾ Example: {train_data[0]}")

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("gpt2")

def tokenize_function(example):
    return tokenizer(example["text"], truncation=True)

tokenized_train = train_data.map(tokenize_function, batched=True)
tokenized_val = val_data.map(tokenize_function, batched=True)

from transformers import AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling
import os

# Disable WandB and other logging that asks for API keys
os.environ["WANDB_DISABLED"] = "true"
os.environ["TRANSFORMERS_NO_WANDB"] = "1"

model = AutoModelForCausalLM.from_pretrained("gpt2")

training_args = TrainingArguments(
    output_dir="./gpt2-finetuned",
    eval_strategy="epoch", # Changed from evaluation_strategy to eval_strategy
    logging_strategy="steps",
    logging_steps=10,
    num_train_epochs=1,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    save_strategy="no",  # donâ€™t save to hub
    report_to=[],        # no WandB, no TensorBoard
)

# Set the padding token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train.select(range(200)),  # small subset to test quickly
    eval_dataset=tokenized_val.select(range(100)),
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()

import math

eval_results = trainer.evaluate()
print(f"Perplexity: {math.exp(eval_results['eval_loss']):.2f}")

import torch
from tqdm import tqdm
from transformers import GPT2Tokenizer, GPT2LMHeadModel

def compute_top_k_accuracy(dataset, model, tokenizer, k=5, num_samples=100):
    model.eval()
    correct = 0
    total = 0

    for sample in tqdm(dataset.select(range(num_samples))):
        text = sample["text"]
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
        input_ids = inputs["input_ids"].to(model.device)

        # Skip short inputs
        if input_ids.shape[1] < 2:
            continue

        with torch.no_grad():
            outputs = model(input_ids)
            logits = outputs.logits

        # Get the predicted logits for each token in the sequence
        for i in range(input_ids.shape[1] - 1):
            next_token_logits = logits[0, i]
            true_next_token = input_ids[0, i + 1]

            # Top-k predictions
            top_k_tokens = torch.topk(next_token_logits, k).indices

            if true_next_token in top_k_tokens:
                correct += 1
            total += 1

    accuracy = correct / total if total > 0 else 0
    return accuracy

top_k_acc = compute_top_k_accuracy(val_data, model, tokenizer, k=5, num_samples=100)
print(f"Top-5 Accuracy: {top_k_acc * 100:.2f}%")

input_text = "he opened the door and saw"
input_ids = tokenizer(input_text, return_tensors="pt").input_ids.to(model.device)

with torch.no_grad():
    output = model.generate(input_ids, max_new_tokens=1, do_sample=True)

predicted_text = tokenizer.decode(output[0])
print("Next word prediction:", predicted_text)

